{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c2dba7",
   "metadata": {},
   "source": [
    "# Prepara CSV por arquivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd3082",
   "metadata": {},
   "source": [
    "## CONSTANTES DE CONFIGURAÇÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0e200a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Iterator, Dict, List\n",
    "from pynwb import NWBFile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from pynwb import NWBHDF5IO\n",
    "from pynwb.file import NWBFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b403773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"arq\")                      # onde estão os .nwb\n",
    "INDEX_JSON = Path(\"index.json\")             # índice que lista tasks -> arquivos .nwb\n",
    "OUTPUT_ROOT = Path(\"outputs_by_task\")       # saída por task (subpastas)\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "CONSOLIDATED_DIR = Path(\"nwb_consolidated_csv_by_task\")\n",
    "CONSOLIDATED_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172b986f",
   "metadata": {},
   "source": [
    "## Funções Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5beaa886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_nwb_files(data_dir: Path) -> List[Path]:\n",
    "    \"\"\"Lista arquivos .nwb na pasta (recursivo), ordenados.\"\"\"\n",
    "    return sorted([p for p in Path(data_dir).rglob(\"*.nwb\")])\n",
    "\n",
    "def open_nwb(nwb_path: Path) -> Tuple[NWBHDF5IO, NWBFile]:\n",
    "    \"\"\"\n",
    "    Abre o NWB e retorna (io, nwbfile).\n",
    "    IMPORTANTE: chame io.close() quando terminar.\n",
    "    \"\"\"\n",
    "    io = NWBHDF5IO(str(nwb_path), mode=\"r\", load_namespaces=True)\n",
    "    nwb = io.read()\n",
    "    return io, nwb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5054695f",
   "metadata": {},
   "source": [
    "## Metadados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2e859077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epochs_df(nwbfile: NWBFile) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"DataFrame CRU de epochs, ou None se não houver.\"\"\"\n",
    "    epochs = getattr(nwbfile, \"epochs\", None)\n",
    "    return None if epochs is None else epochs.to_dataframe()\n",
    "\n",
    "def get_units_table(nwbfile: NWBFile) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"DataFrame CRU de units (todos os campos presentes no NWB), sem filtrar.\"\"\"\n",
    "    units = getattr(nwbfile, \"units\", None)\n",
    "    return None if units is None else units.to_dataframe()\n",
    "\n",
    "def get_session_meta(nwbfile: NWBFile) -> Dict[str, object]:\n",
    "    \"\"\"Metadados básicos da sessão (crus).\"\"\"\n",
    "    subj = getattr(nwbfile, \"subject\", None)\n",
    "    return {\n",
    "        \"identifier\": nwbfile.identifier,\n",
    "        \"session_description\": nwbfile.session_description,\n",
    "        \"session_start_time\": nwbfile.session_start_time,\n",
    "        \"timestamps_reference_time\": nwbfile.timestamps_reference_time,\n",
    "        \"subject_id\": getattr(subj, \"subject_id\", None) if subj else None,\n",
    "        \"subject_description\": getattr(subj, \"description\", None) if subj else None,\n",
    "        \"institution\": getattr(nwbfile, \"institution\", None),\n",
    "        \"lab\": getattr(nwbfile, \"lab\", None),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a43c9d8",
   "metadata": {},
   "source": [
    "# Handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "47bca084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spike_handles(nwbfile: NWBFile) -> Tuple[Optional[object], Optional[object], Optional[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Retorna *handles* HDF5 para leitura preguiçosa de spikes:\n",
    "    (ds_times, ds_index, unit_ids) ou (None, None, None).\n",
    "    \"\"\"\n",
    "    units = getattr(nwbfile, \"units\", None)\n",
    "    if units is None or len(units) == 0:\n",
    "        return None, None, None\n",
    "    ds_times = units[\"spike_times\"].data\n",
    "    ds_index = units[\"spike_times_index\"].data\n",
    "    unit_ids = units.id.data[:]\n",
    "    return ds_times, ds_index, unit_ids\n",
    "\n",
    "def get_epochs_df(nwbfile: NWBFile) -> Optional[pd.DataFrame]:\n",
    "    epochs = getattr(nwbfile, \"epochs\", None)\n",
    "    return None if epochs is None else epochs.to_dataframe()\n",
    "\n",
    "\n",
    "def iter_unit_spike_slices(nwbfile: NWBFile) -> Iterator[Tuple[int, int, int, int]]:\n",
    "    \"\"\"Itera (pos, unit_id, start_idx, stop_idx) para fatiar ds_times[start:stop].\"\"\"\n",
    "    ds_times, ds_index, unit_ids = get_spike_handles(nwbfile)\n",
    "    if ds_times is None:\n",
    "        return\n",
    "    start = 0\n",
    "    for i, stop in enumerate(ds_index[:]):\n",
    "        yield i, int(unit_ids[i]), int(start), int(stop)\n",
    "        start = int(stop)\n",
    "\n",
    "def load_unit_spike_times(ds_times, start_idx: int, stop_idx: int) -> np.ndarray:\n",
    "    \"\"\"Carrega os spikes de UMA unidade (fatia do dataset).\"\"\"\n",
    "    return np.asarray(ds_times[start_idx:stop_idx])\n",
    "\n",
    "\n",
    "def get_behavior_interfaces(nwbfile: NWBFile) -> Dict[str, object]:\n",
    "    \"\"\"Lista as interfaces de comportamento dentro de processing/behavior.\"\"\"\n",
    "    beh = nwbfile.processing.get(\"behavior\", None)\n",
    "    return {} if beh is None else dict(beh.data_interfaces.items())\n",
    "\n",
    "def get_behavior_time_series(\n",
    "    nwbfile: NWBFile,\n",
    "    name_filter: Optional[str] = None\n",
    ") -> List[Dict[str, object]]:\n",
    "    \"\"\"\n",
    "    Retorna lista de séries de comportamento cruas.\n",
    "    Cada item = {\"name\", \"data\", \"timestamps\", \"rate\", \"raw_obj\"}.\n",
    "    \"\"\"\n",
    "    out: List[Dict[str, object]] = []\n",
    "    for name, obj in get_behavior_interfaces(nwbfile).items():\n",
    "        # Containers com múltiplas séries\n",
    "        if hasattr(obj, \"time_series\") and obj.time_series:\n",
    "            for ts_name, ts in obj.time_series.items():\n",
    "                nm = ts.name or ts_name or name\n",
    "                if name_filter and name_filter.lower() not in nm.lower():\n",
    "                    continue\n",
    "                data = np.asarray(ts.data)\n",
    "                t = np.asarray(ts.timestamps) if ts.timestamps is not None else None\n",
    "                out.append({\"name\": nm, \"data\": data, \"timestamps\": t,\n",
    "                            \"rate\": getattr(ts, \"rate\", None), \"raw_obj\": ts})\n",
    "        # Séries espaciais\n",
    "        if hasattr(obj, \"spatial_series\") and obj.spatial_series:\n",
    "            for ts_name, ts in obj.spatial_series.items():\n",
    "                nm = ts.name or ts_name or name\n",
    "                if name_filter and name_filter.lower() not in nm.lower():\n",
    "                    continue\n",
    "                data = np.asarray(ts.data)\n",
    "                t = np.asarray(ts.timestamps) if ts.timestamps is not None else None\n",
    "                out.append({\"name\": nm, \"data\": data, \"timestamps\": t,\n",
    "                            \"rate\": getattr(ts, \"rate\", None), \"raw_obj\": ts})\n",
    "        # Série direta\n",
    "        if hasattr(obj, \"data\"):\n",
    "            nm = getattr(obj, \"name\", None) or name\n",
    "            if (name_filter is None) or (name_filter.lower() in (nm or \"\").lower()):\n",
    "                data = np.asarray(obj.data)\n",
    "                t = np.asarray(obj.timestamps) if getattr(obj, \"timestamps\", None) is not None else None\n",
    "                out.append({\"name\": nm, \"data\": data, \"timestamps\": t,\n",
    "                            \"rate\": getattr(obj, \"rate\", None), \"raw_obj\": obj})\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011afb07",
   "metadata": {},
   "source": [
    "## Spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b372f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_spike_times_to_csv(\n",
    "    nwbfile: NWBFile,\n",
    "    output_dir: Path,\n",
    "    nwb_name: str,\n",
    "    extra_cols: Optional[Dict[str, object]] = None,\n",
    ") -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Usa seus helpers:\n",
    "      - get_spike_handles(nwbfile)\n",
    "      - iter_unit_spike_slices(nwbfile)\n",
    "      - load_unit_spike_times(ds_times, a, b)\n",
    "    Gera: <nwb_name>_spike_times.csv com colunas [file_id, unit_id, spike_time, ...extras].\n",
    "    \"\"\"\n",
    "    extra_cols = extra_cols or {}\n",
    "    ds_times, ds_index, unit_ids = get_spike_handles(nwbfile)\n",
    "    if ds_times is None or ds_index is None:\n",
    "        print(\"   Sem spikes (spike_times/spike_times_index ausentes).\")\n",
    "        return None\n",
    "\n",
    "    file_id = nwbfile.identifier or nwb_name\n",
    "    rows: List[pd.DataFrame] = []\n",
    "\n",
    "    for _, uid, a, b in iter_unit_spike_slices(nwbfile):\n",
    "        st_u = load_unit_spike_times(ds_times, a, b)\n",
    "        if st_u.size == 0:\n",
    "            continue\n",
    "        df_u = pd.DataFrame({\"file_id\": file_id, \"unit_id\": int(uid), \"spike_time\": st_u})\n",
    "        for k, v in extra_cols.items():\n",
    "            df_u[k] = v\n",
    "        rows.append(df_u)\n",
    "\n",
    "    if not rows:\n",
    "        print(\"   Sem spikes para salvar (tabela vazia).\")\n",
    "        return None\n",
    "\n",
    "    df = pd.concat(rows, ignore_index=True)\n",
    "    out = Path(output_dir) / f\"{nwb_name}_spike_times.csv\"\n",
    "    df.to_csv(out, index=False)\n",
    "    print(f\"   Spikes salvos: {len(df):,} -> {out.name}\")\n",
    "    return out\n",
    "\n",
    "def save_behavior_series_to_csv(\n",
    "    nwbfile: NWBFile,\n",
    "    output_dir: Path,\n",
    "    nwb_name: str,\n",
    "    extra_cols: Optional[Dict[str, object]] = None,\n",
    "    name_filter: Optional[str] = None,\n",
    ") -> List[Path]:\n",
    "    \"\"\"\n",
    "    Usa seu get_behavior_time_series(...) para exportar cada série:\n",
    "    - Se 1D -> 1 coluna com o nome\n",
    "    - Se 2D -> name_dim0, name_dim1, ...\n",
    "    Inclui timestamps quando existirem.\n",
    "    \"\"\"\n",
    "    extra_cols = extra_cols or {}\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    saved = []\n",
    "    series_list = get_behavior_time_series(nwbfile, name_filter=name_filter)\n",
    "\n",
    "    for s in series_list:\n",
    "        name = s[\"name\"]\n",
    "        data = np.asarray(s[\"data\"])\n",
    "        t = np.asarray(s[\"timestamps\"]) if s[\"timestamps\"] is not None else None\n",
    "\n",
    "        # constrói DF\n",
    "        if t is not None:\n",
    "            df = pd.DataFrame({\"timestamp\": t})\n",
    "        else:\n",
    "            df = pd.DataFrame({\"idx\": np.arange(len(data))})\n",
    "\n",
    "        if data.ndim == 1:\n",
    "            df[name] = data\n",
    "        elif data.ndim == 2:\n",
    "            for j in range(data.shape[1]):\n",
    "                df[f\"{name}_dim{j}\"] = data[:, j]\n",
    "        else:\n",
    "            continue  # ignorar nd>2 para manter simples\n",
    "\n",
    "        for k, v in extra_cols.items():\n",
    "            df[k] = v\n",
    "\n",
    "        safe = name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\" \", \"_\")\n",
    "        out = output_dir / f\"{nwb_name}__beh_{safe}.csv\"\n",
    "        df.to_csv(out, index=False)\n",
    "        saved.append(out)\n",
    "\n",
    "    print(f\"   Séries de comportamento salvas: {len(saved)} arquivo(s)\")\n",
    "    return saved\n",
    "\n",
    "def save_epochs_to_csv(\n",
    "    nwbfile: NWBFile,\n",
    "    output_dir: Path,\n",
    "    nwb_name: str,\n",
    "    extra_cols: Optional[Dict[str, object]] = None,\n",
    ") -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Exporta a tabela de epochs (se existir) para CSV único.\n",
    "    Usa seu get_epochs_df(nwbfile).\n",
    "    Nome: <nwb_name>__epochs.csv\n",
    "    \"\"\"\n",
    "    extra_cols = extra_cols or {}\n",
    "    df = get_epochs_df(nwbfile)\n",
    "    if df is None or df.empty:\n",
    "        print(\"   Nenhum epoch para salvar.\")\n",
    "        return None\n",
    "\n",
    "    # garante coluna id sequencial (o .to_dataframe geralmente já traz o index)\n",
    "    df_out = df.reset_index(drop=False).rename(columns={\"index\": \"epoch_id\"})\n",
    "    for k, v in extra_cols.items():\n",
    "        df_out[k] = v\n",
    "\n",
    "    out = Path(output_dir) / f\"{nwb_name}__epochs.csv\"\n",
    "    df_out.to_csv(out, index=False)\n",
    "    print(f\"   Epochs salvos: {len(df_out):,} -> {out.name}\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d92b7f1",
   "metadata": {},
   "source": [
    "## Loop principal: iterar por task e arquivo da task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b6cb3bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Task: Foraging task | Arquivo: 26863_2020-Nov-04_12-46-06.nwb ===\n",
      "   Spikes salvos: 1,072 -> 26863_2020-Nov-04_12-46-06__task-Foraging_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 26863_2020-Nov-04_12-46-06__task-Foraging_task__epochs.csv\n",
      "\n",
      "=== Task: Foraging task | Arquivo: 26863_2020-Nov-05_10-26-23.nwb ===\n",
      "   Spikes salvos: 721 -> 26863_2020-Nov-05_10-26-23__task-Foraging_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 26863_2020-Nov-05_10-26-23__task-Foraging_task__epochs.csv\n",
      "\n",
      "=== Task: Foraging task | Arquivo: 26965_2020-Dec-07_15-52-32.nwb ===\n",
      "   Spikes salvos: 393 -> 26965_2020-Dec-07_15-52-32__task-Foraging_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 26965_2020-Dec-07_15-52-32__task-Foraging_task__epochs.csv\n",
      "\n",
      "=== Task: Foraging task | Arquivo: 26965_2020-Dec-08_11-29-48.nwb ===\n",
      "   Spikes salvos: 240 -> 26965_2020-Dec-08_11-29-48__task-Foraging_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 26965_2020-Dec-08_11-29-48__task-Foraging_task__epochs.csv\n",
      "\n",
      "=== Task: Foraging task | Arquivo: 26965_2020-Dec-08_14-36-11.nwb ===\n",
      "   Spikes salvos: 548 -> 26965_2020-Dec-08_14-36-11__task-Foraging_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 26965_2020-Dec-08_14-36-11__task-Foraging_task__epochs.csv\n",
      "\n",
      "=== Task: Foraging task | Arquivo: 26965_2020-Dec-09_12-10-58.nwb ===\n",
      "   Spikes salvos: 548 -> 26965_2020-Dec-09_12-10-58__task-Foraging_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 26965_2020-Dec-09_12-10-58__task-Foraging_task__epochs.csv\n",
      "\n",
      "=== Task: Foraging task | Arquivo: 26965_2020-Dec-11_10-58-11.nwb ===\n",
      "   Spikes salvos: 544 -> 26965_2020-Dec-11_10-58-11__task-Foraging_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 26965_2020-Dec-11_10-58-11__task-Foraging_task__epochs.csv\n",
      "\n",
      "=== Task: Foraging task | Arquivo: 26966_2020-Dec-14_15-18-10.nwb ===\n",
      "   Spikes salvos: 477 -> 26966_2020-Dec-14_15-18-10__task-Foraging_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 26966_2020-Dec-14_15-18-10__task-Foraging_task__epochs.csv\n",
      "\n",
      "=== Task: Foraging task | Arquivo: 26966_2020-Dec-16_11-48-01.nwb ===\n",
      "   Spikes salvos: 540 -> 26966_2020-Dec-16_11-48-01__task-Foraging_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 26966_2020-Dec-16_11-48-01__task-Foraging_task__epochs.csv\n",
      "\n",
      "=== Task: Foraging task | Arquivo: 27284_2021-Jun-15_19-16-52.nwb ===\n",
      "   Spikes salvos: 2,306 -> 27284_2021-Jun-15_19-16-52__task-Foraging_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 2 -> 27284_2021-Jun-15_19-16-52__task-Foraging_task__epochs.csv\n",
      "\n",
      "=== Task: Foraging task | Arquivo: 27284_2021-Jun-16_14-33-11.nwb ===\n",
      "   Spikes salvos: 2,772 -> 27284_2021-Jun-16_14-33-11__task-Foraging_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 27284_2021-Jun-16_14-33-11__task-Foraging_task__epochs.csv\n",
      "\n",
      "=== Task: Foraging task | Arquivo: 27284_2021-Jun-17_15-01-36.nwb ===\n",
      "   Spikes salvos: 1,639 -> 27284_2021-Jun-17_15-01-36__task-Foraging_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 27284_2021-Jun-17_15-01-36__task-Foraging_task__epochs.csv\n",
      "\n",
      "=== Task: Foraging task | Arquivo: 27284_2021-Jun-18_11-01-41.nwb ===\n",
      "   Spikes salvos: 1,226 -> 27284_2021-Jun-18_11-01-41__task-Foraging_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 27284_2021-Jun-18_11-01-41__task-Foraging_task__epochs.csv\n",
      "\n",
      "=== Task: Foraging task | Arquivo: 27285_2021-Jul-01_13-26-22.nwb ===\n",
      "   Spikes salvos: 951 -> 27285_2021-Jul-01_13-26-22__task-Foraging_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 27285_2021-Jul-01_13-26-22__task-Foraging_task__epochs.csv\n",
      "\n",
      "=== Task: Foraging task | Arquivo: 27285_2021-Jul-02_13-23-12.nwb ===\n",
      "   Spikes salvos: 875 -> 27285_2021-Jul-02_13-23-12__task-Foraging_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 27285_2021-Jul-02_13-23-12__task-Foraging_task__epochs.csv\n",
      "\n",
      "=== Task: Foraging task | Arquivo: 27285_2021-Jun-30_13-59-11.nwb ===\n",
      "   Spikes salvos: 2,253 -> 27285_2021-Jun-30_13-59-11__task-Foraging_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 2 -> 27285_2021-Jun-30_13-59-11__task-Foraging_task__epochs.csv\n",
      "\n",
      "=== Task: Foraging task | Arquivo: 27963_2022-Feb-08_21-14-27.nwb ===\n",
      "   Spikes salvos: 1,890 -> 27963_2022-Feb-08_21-14-27__task-Foraging_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 27963_2022-Feb-08_21-14-27__task-Foraging_task__epochs.csv\n",
      "\n",
      "=== Task: Foraging task | Arquivo: 27963_2022-Feb-10_11-06-56.nwb ===\n",
      "   Spikes salvos: 1,357 -> 27963_2022-Feb-10_11-06-56__task-Foraging_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 4 -> 27963_2022-Feb-10_11-06-56__task-Foraging_task__epochs.csv\n",
      "\n",
      "=== Task: Foraging task | Arquivo: 27963_2022-Feb-10_14-07-47.nwb ===\n",
      "   Spikes salvos: 1,288 -> 27963_2022-Feb-10_14-07-47__task-Foraging_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 4 -> 27963_2022-Feb-10_14-07-47__task-Foraging_task__epochs.csv\n",
      "\n",
      "=== Task: Foraging task | Arquivo: 28003_2022-Mar-07_20-44-03.nwb ===\n",
      "   Spikes salvos: 895 -> 28003_2022-Mar-07_20-44-03__task-Foraging_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 28003_2022-Mar-07_20-44-03__task-Foraging_task__epochs.csv\n",
      "\n",
      "=== Task: Foraging task | Arquivo: 28003_2022-Mar-08_11-22-53.nwb ===\n",
      "   Spikes salvos: 1,193 -> 28003_2022-Mar-08_11-22-53__task-Foraging_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 28003_2022-Mar-08_11-22-53__task-Foraging_task__epochs.csv\n",
      "\n",
      "=== Task: Foraging task | Arquivo: 28003_2022-Mar-08_11-38-59.nwb ===\n",
      "   Spikes salvos: 1,056 -> 28003_2022-Mar-08_11-38-59__task-Foraging_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 28003_2022-Mar-08_11-38-59__task-Foraging_task__epochs.csv\n",
      "\n",
      "=== Task: Foraging task | Arquivo: 28003_2022-Mar-08_15-04-04.nwb ===\n",
      "   Spikes salvos: 1,386 -> 28003_2022-Mar-08_15-04-04__task-Foraging_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 4 -> 28003_2022-Mar-08_15-04-04__task-Foraging_task__epochs.csv\n",
      "\n",
      "=== Task: Foraging task | Arquivo: 28003_2022-Mar-10_10-53-58.nwb ===\n",
      "   Spikes salvos: 834 -> 28003_2022-Mar-10_10-53-58__task-Foraging_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 4 -> 28003_2022-Mar-10_10-53-58__task-Foraging_task__epochs.csv\n",
      "\n",
      "=== Task: Figure-eight task | Arquivo: 26965_2020-Dec-07_16-51-52.nwb ===\n",
      "   Spikes salvos: 393 -> 26965_2020-Dec-07_16-51-52__task-Figure-eight_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 26965_2020-Dec-07_16-51-52__task-Figure-eight_task__epochs.csv\n",
      "\n",
      "=== Task: Figure-eight task | Arquivo: 26965_2020-Dec-08_10-46-12.nwb ===\n",
      "   Spikes salvos: 393 -> 26965_2020-Dec-08_10-46-12__task-Figure-eight_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 26965_2020-Dec-08_10-46-12__task-Figure-eight_task__epochs.csv\n",
      "\n",
      "=== Task: Figure-eight task | Arquivo: 26965_2020-Dec-09_11-52-59.nwb ===\n",
      "   Spikes salvos: 544 -> 26965_2020-Dec-09_11-52-59__task-Figure-eight_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 26965_2020-Dec-09_11-52-59__task-Figure-eight_task__epochs.csv\n",
      "\n",
      "=== Task: Figure-eight task | Arquivo: 26966_2020-Dec-14_16-26-40.nwb ===\n",
      "   Spikes salvos: 477 -> 26966_2020-Dec-14_16-26-40__task-Figure-eight_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 26966_2020-Dec-14_16-26-40__task-Figure-eight_task__epochs.csv\n",
      "\n",
      "=== Task: Figure-eight task | Arquivo: 26966_2020-Dec-15_11-47-50.nwb ===\n",
      "   Spikes salvos: 477 -> 26966_2020-Dec-15_11-47-50__task-Figure-eight_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 26966_2020-Dec-15_11-47-50__task-Figure-eight_task__epochs.csv\n",
      "\n",
      "=== Task: Natural sleep | Arquivo: 27284_2021-Jun-16_08-24-12.nwb ===\n",
      "   Spikes salvos: 2,537 -> 27284_2021-Jun-16_08-24-12__task-Natural_sleep_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 27284_2021-Jun-16_08-24-12__task-Natural_sleep__epochs.csv\n",
      "\n",
      "=== Task: Natural sleep | Arquivo: 27285_2021-Jul-01_08-10-45.nwb ===\n",
      "   Spikes salvos: 8,703 -> 27285_2021-Jul-01_08-10-45__task-Natural_sleep_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 4 -> 27285_2021-Jul-01_08-10-45__task-Natural_sleep__epochs.csv\n",
      "\n",
      "=== Task: Natural sleep | Arquivo: 27963_2022-Feb-09_08-02-57.nwb ===\n",
      "   Spikes salvos: 2,312 -> 27963_2022-Feb-09_08-02-57__task-Natural_sleep_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 27963_2022-Feb-09_08-02-57__task-Natural_sleep__epochs.csv\n",
      "\n",
      "=== Task: Natural sleep | Arquivo: 28003_2022-Mar-09_07-54-00.nwb ===\n",
      "   Spikes salvos: 1,598 -> 28003_2022-Mar-09_07-54-00__task-Natural_sleep_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 28003_2022-Mar-09_07-54-00__task-Natural_sleep__epochs.csv\n",
      "\n",
      "=== Task: Natural sleep | Arquivo: 29629_2024-Oct-30_05-40-15.nwb ===\n",
      "   Spikes salvos: 1,009 -> 29629_2024-Oct-30_05-40-15__task-Natural_sleep_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 29629_2024-Oct-30_05-40-15__task-Natural_sleep__epochs.csv\n",
      "\n",
      "=== Task: Natural sleep | Arquivo: 29630_2024-Sep-17_07-08-26.nwb ===\n",
      "   Spikes salvos: 2,728 -> 29630_2024-Sep-17_07-08-26__task-Natural_sleep_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 29630_2024-Sep-17_07-08-26__task-Natural_sleep__epochs.csv\n",
      "\n",
      "=== Task: Natural sleep | Arquivo: 29630_2024-Sep-17_23-09-30.nwb ===\n",
      "   Spikes salvos: 2,776 -> 29630_2024-Sep-17_23-09-30__task-Natural_sleep_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 29630_2024-Sep-17_23-09-30__task-Natural_sleep__epochs.csv\n",
      "\n",
      "=== Task: Natural sleep | Arquivo: 29630_2024-Sep-19_05-09-20.nwb ===\n",
      "   Spikes salvos: 2,195 -> 29630_2024-Sep-19_05-09-20__task-Natural_sleep_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 1 -> 29630_2024-Sep-19_05-09-20__task-Natural_sleep__epochs.csv\n",
      "\n",
      "=== Task: Odor sequence task | Arquivo: 27285_2021-Jul-01_08-10-45.nwb ===\n",
      "   Spikes salvos: 8,703 -> 27285_2021-Jul-01_08-10-45__task-Odor_sequence_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 4 -> 27285_2021-Jul-01_08-10-45__task-Odor_sequence_task__epochs.csv\n",
      "\n",
      "=== Task: Odor sequence task | Arquivo: 27285_2021-Jul-01_12-33-30.nwb ===\n",
      "   Spikes salvos: 8,703 -> 27285_2021-Jul-01_12-33-30__task-Odor_sequence_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 3 -> 27285_2021-Jul-01_12-33-30__task-Odor_sequence_task__epochs.csv\n",
      "\n",
      "=== Task: Odor sequence task | Arquivo: 27285_2021-Jun-30_10-03-39.nwb ===\n",
      "   Spikes salvos: 9,578 -> 27285_2021-Jun-30_10-03-39__task-Odor_sequence_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 3 -> 27285_2021-Jun-30_10-03-39__task-Odor_sequence_task__epochs.csv\n",
      "\n",
      "=== Task: Odor sequence task | Arquivo: 27285_2021-Jun-30_13-05-14.nwb ===\n",
      "   Spikes salvos: 9,578 -> 27285_2021-Jun-30_13-05-14__task-Odor_sequence_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 3 -> 27285_2021-Jun-30_13-05-14__task-Odor_sequence_task__epochs.csv\n",
      "\n",
      "=== Task: Odor sequence task | Arquivo: 29630_2024-Sep-17_13-26-08.nwb ===\n",
      "   Spikes salvos: 6,978 -> 29630_2024-Sep-17_13-26-08__task-Odor_sequence_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 3 -> 29630_2024-Sep-17_13-26-08__task-Odor_sequence_task__epochs.csv\n",
      "\n",
      "=== Task: Odor sequence task | Arquivo: 29630_2024-Sep-18_14-20-12.nwb ===\n",
      "   Spikes salvos: 4,732 -> 29630_2024-Sep-18_14-20-12__task-Odor_sequence_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 3 -> 29630_2024-Sep-18_14-20-12__task-Odor_sequence_task__epochs.csv\n",
      "\n",
      "=== Task: Odor sequence task | Arquivo: 29630_2024-Sep-19_11-15-17.nwb ===\n",
      "   Spikes salvos: 5,029 -> 29630_2024-Sep-19_11-15-17__task-Odor_sequence_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 3 -> 29630_2024-Sep-19_11-15-17__task-Odor_sequence_task__epochs.csv\n",
      "\n",
      "=== Task: Odor sequence task | Arquivo: 29630_2024-Sep-20_10-32-21.nwb ===\n",
      "   Spikes salvos: 5,744 -> 29630_2024-Sep-20_10-32-21__task-Odor_sequence_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 3 -> 29630_2024-Sep-20_10-32-21__task-Odor_sequence_task__epochs.csv\n",
      "\n",
      "=== Task: Odor sequence task | Arquivo: 29630_2024-Sep-20_13-20-20.nwb ===\n",
      "   Spikes salvos: 4,461 -> 29630_2024-Sep-20_13-20-20__task-Odor_sequence_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 3 -> 29630_2024-Sep-20_13-20-20__task-Odor_sequence_task__epochs.csv\n",
      "\n",
      "=== Task: Odor sequence task | Arquivo: 29630_2024-Sep-24_14-25-53.nwb ===\n",
      "   Spikes salvos: 6,523 -> 29630_2024-Sep-24_14-25-53__task-Odor_sequence_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 3 -> 29630_2024-Sep-24_14-25-53__task-Odor_sequence_task__epochs.csv\n",
      "\n",
      "=== Task: Novel object task | Arquivo: 27963_2022-Feb-10_11-06-56.nwb ===\n",
      "   Spikes salvos: 1,357 -> 27963_2022-Feb-10_11-06-56__task-Novel_object_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 4 -> 27963_2022-Feb-10_11-06-56__task-Novel_object_task__epochs.csv\n",
      "\n",
      "=== Task: Novel object task | Arquivo: 27963_2022-Feb-10_14-07-47.nwb ===\n",
      "   Spikes salvos: 1,288 -> 27963_2022-Feb-10_14-07-47__task-Novel_object_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 4 -> 27963_2022-Feb-10_14-07-47__task-Novel_object_task__epochs.csv\n",
      "\n",
      "=== Task: Novel object task | Arquivo: 28003_2022-Mar-08_15-04-04.nwb ===\n",
      "   Spikes salvos: 1,386 -> 28003_2022-Mar-08_15-04-04__task-Novel_object_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 4 -> 28003_2022-Mar-08_15-04-04__task-Novel_object_task__epochs.csv\n",
      "\n",
      "=== Task: Novel object task | Arquivo: 28003_2022-Mar-10_10-53-58.nwb ===\n",
      "   Spikes salvos: 834 -> 28003_2022-Mar-10_10-53-58__task-Novel_object_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 4 -> 28003_2022-Mar-10_10-53-58__task-Novel_object_task__epochs.csv\n",
      "\n",
      "=== Task: Novel object task | Arquivo: 29629_2024-Oct-29_15-05-13.nwb ===\n",
      "   Spikes salvos: 380 -> 29629_2024-Oct-29_15-05-13__task-Novel_object_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 4 -> 29629_2024-Oct-29_15-05-13__task-Novel_object_task__epochs.csv\n",
      "\n",
      "=== Task: Novel object task | Arquivo: 29629_2024-Oct-30_15-13-05.nwb ===\n",
      "   Spikes salvos: 396 -> 29629_2024-Oct-30_15-13-05__task-Novel_object_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 4 -> 29629_2024-Oct-30_15-13-05__task-Novel_object_task__epochs.csv\n",
      "\n",
      "=== Task: Novel object task | Arquivo: 29630_2024-Sep-17_09-22-08.nwb ===\n",
      "   Spikes salvos: 2,119 -> 29630_2024-Sep-17_09-22-08__task-Novel_object_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 4 -> 29630_2024-Sep-17_09-22-08__task-Novel_object_task__epochs.csv\n",
      "\n",
      "=== Task: Novel object task | Arquivo: 29630_2024-Sep-18_10-18-21.nwb ===\n",
      "   Spikes salvos: 1,918 -> 29630_2024-Sep-18_10-18-21__task-Novel_object_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 4 -> 29630_2024-Sep-18_10-18-21__task-Novel_object_task__epochs.csv\n",
      "\n",
      "=== Task: Novel object task | Arquivo: 29630_2024-Sep-18_15-19-16.nwb ===\n",
      "   Spikes salvos: 2,246 -> 29630_2024-Sep-18_15-19-16__task-Novel_object_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 4 -> 29630_2024-Sep-18_15-19-16__task-Novel_object_task__epochs.csv\n",
      "\n",
      "=== Task: Novel object task | Arquivo: 29630_2024-Sep-20_09-49-05.nwb ===\n",
      "   Spikes salvos: 1,749 -> 29630_2024-Sep-20_09-49-05__task-Novel_object_task_spike_times.csv\n",
      "   Séries de comportamento salvas: 4 arquivo(s)\n",
      "   Epochs salvos: 4 -> 29630_2024-Sep-20_09-49-05__task-Novel_object_task__epochs.csv\n",
      "\n",
      "✔️ Loop por task concluído. Saídas em: D:\\UFPB\\replay_curve\\replay_curve\\outputs_by_task\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "DATA_DIR = Path(\"arq\")               # onde estão seus .nwb\n",
    "INDEX_JSON = Path(\"index.json\")      # mapeia task -> arquivos\n",
    "OUTPUT_ROOT = Path(\"outputs_by_task\")\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _slug(s: str) -> str:\n",
    "    return (str(s).replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "                 .replace(\" \", \"_\").replace(\":\", \"-\"))\n",
    "\n",
    "# Carrega índice\n",
    "with open(INDEX_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    idx = json.load(f)\n",
    "\n",
    "task_index: dict = idx.get(\"task_index\", {})\n",
    "\n",
    "# Mapeia arquivos existentes\n",
    "available = {p.name: p for p in list_nwb_files(DATA_DIR)}\n",
    "\n",
    "for task_name, files_map in task_index.items():\n",
    "    task_slug = _slug(task_name)\n",
    "    out_dir = OUTPUT_ROOT / task_slug\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    expected = set(files_map.keys())\n",
    "    missing = sorted(expected - set(available.keys()))\n",
    "    if missing:\n",
    "        print(f\"[{task_name}] faltando {len(missing)} arquivo(s) em {DATA_DIR}: {missing[:3]} ...\")\n",
    "\n",
    "    for fname in sorted(expected & set(available.keys())):\n",
    "        nwb_path = available[fname]\n",
    "        nwb_name = nwb_path.stem\n",
    "        print(f\"\\n=== Task: {task_name} | Arquivo: {fname} ===\")\n",
    "\n",
    "        io, nwb = open_nwb(nwb_path)\n",
    "        try:\n",
    "            # Metadados\n",
    "            meta = get_session_meta(nwb)\n",
    "            meta_df = pd.DataFrame([meta])\n",
    "            meta_df.insert(0, \"task\", task_name)\n",
    "            meta_df.insert(0, \"source_file\", fname)\n",
    "            meta_df.to_csv(out_dir / f\"{nwb_name}__task-{task_slug}__meta.csv\", index=False)\n",
    "\n",
    "            # Units CRU\n",
    "            df_units = get_units_table(nwb)\n",
    "            if df_units is not None and not df_units.empty:\n",
    "                dfu = df_units.copy()\n",
    "                dfu.insert(0, \"task\", task_name)\n",
    "                dfu.insert(0, \"source_file\", fname)\n",
    "                dfu.to_csv(out_dir / f\"{nwb_name}__task-{task_slug}__units.csv\", index=False)\n",
    "\n",
    "            # Spikes\n",
    "            save_spike_times_to_csv(\n",
    "                nwbfile=nwb,\n",
    "                output_dir=out_dir,\n",
    "                nwb_name=f\"{nwb_name}__task-{task_slug}\",\n",
    "                extra_cols={\"source_file\": fname, \"task\": task_name}\n",
    "            )\n",
    "\n",
    "            # Séries comportamento (opcional: filtrar por \"speed\" ou \"position\")\n",
    "            save_behavior_series_to_csv(\n",
    "                nwbfile=nwb,\n",
    "                output_dir=out_dir,\n",
    "                nwb_name=f\"{nwb_name}__task-{task_slug}\",\n",
    "                extra_cols={\"source_file\": fname, \"task\": task_name},\n",
    "                name_filter=None  # ou \"speed\" / \"position\"\n",
    "            )\n",
    "\n",
    "            # Epochs\n",
    "            save_epochs_to_csv(\n",
    "                nwbfile=nwb,\n",
    "                output_dir=out_dir,\n",
    "                nwb_name=f\"{nwb_name}__task-{task_slug}\",\n",
    "                extra_cols={\"source_file\": fname, \"task\": task_name},\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERRO em {fname} ({task_name}): {e}\")\n",
    "        finally:\n",
    "            io.close()\n",
    "\n",
    "print(\"\\n✔️ Loop por task concluído. Saídas em:\", OUTPUT_ROOT.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cca4a1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[meta] SALVO: nwb_consolidated_csv_by_task\\consolidated__meta.csv (57 linhas)\n",
      "[units] SALVO: nwb_consolidated_csv_by_task\\consolidated__units.csv (136,848 linhas)\n",
      "[spike_times] SALVO: nwb_consolidated_csv_by_task\\consolidated__spike_times.csv (136,848 linhas)\n",
      "[epochs] SALVO: nwb_consolidated_csv_by_task\\consolidated__epochs.csv (125 linhas)\n",
      "[behavior_animal_position] SALVO: nwb_consolidated_csv_by_task\\consolidated__behavior_animal_position.csv (16,403,263 linhas)\n",
      "[behavior_head_direction] SALVO: nwb_consolidated_csv_by_task\\consolidated__behavior_head_direction.csv (16,403,263 linhas)\n",
      "[behavior_head_yaw_speed] SALVO: nwb_consolidated_csv_by_task\\consolidated__behavior_head_yaw_speed.csv (16,403,263 linhas)\n",
      "[behavior_movement_speed] SALVO: nwb_consolidated_csv_by_task\\consolidated__behavior_movement_speed.csv (16,403,263 linhas)\n"
     ]
    }
   ],
   "source": [
    "INPUT_ROOT = OUTPUT_ROOT\n",
    "CONSOLIDATED_DIR = Path(\"nwb_consolidated_csv_by_task\")\n",
    "CONSOLIDATED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "groups = {\n",
    "    \"meta\":        \"__meta.csv\",\n",
    "    \"units\":       \"__units.csv\",\n",
    "    \"spike_times\": \"_spike_times.csv\",\n",
    "    \"epochs\":      \"__epochs.csv\",\n",
    "}\n",
    "\n",
    "for group_name, pattern in groups.items():\n",
    "    paths = sorted(INPUT_ROOT.rglob(f\"*{pattern}*\"))\n",
    "    if not paths:\n",
    "        print(f\"[{group_name}] nada encontrado (*{pattern}*) em {INPUT_ROOT}\")\n",
    "        continue\n",
    "\n",
    "    parts = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            df = pd.read_csv(p, low_memory=False)\n",
    "            if \"task\" not in df.columns:\n",
    "                df.insert(0, \"task\", p.parent.name)\n",
    "            if \"source_file\" not in df.columns:\n",
    "                base = p.name.split(\"__task-\")[0]\n",
    "                if not base.endswith(\".nwb\"):\n",
    "                    base += \".nwb\"\n",
    "                df.insert(0, \"source_file\", base)\n",
    "            parts.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"[{group_name}] Falha lendo {p.name}: {e}\")\n",
    "\n",
    "    if not parts:\n",
    "        print(f\"[{group_name}] nenhum arquivo válido para consolidar.\")\n",
    "        continue\n",
    "\n",
    "    final = pd.concat(parts, ignore_index=True)\n",
    "    cols = final.columns.tolist()\n",
    "    for first in [\"source_file\", \"task\"]:\n",
    "        if first in cols:\n",
    "            cols = [first] + [c for c in cols if c != first]\n",
    "    final = final[cols]\n",
    "\n",
    "    out = CONSOLIDATED_DIR / f\"consolidated__{group_name}.csv\"\n",
    "    final.to_csv(out, index=False)\n",
    "    print(f\"[{group_name}] SALVO: {out} ({len(final):,} linhas)\")\n",
    "\n",
    "\n",
    "# comportamento\n",
    "# 2) Consolida BEHAVIOR: um arquivo por série (nome completo \"behavior_*\")\n",
    "beh_paths = sorted(INPUT_ROOT.rglob(\"*__beh_*.csv\"))\n",
    "if not beh_paths:\n",
    "    print(\"[beh] nada encontrado (*__beh_*.csv)\")\n",
    "else:\n",
    "    by_series = defaultdict(list)\n",
    "    for p in beh_paths:\n",
    "        # exemplo: <nwb>__task-<slug>__beh_animal_position.csv\n",
    "        tail = p.name.split(\"__beh_\")[-1]\n",
    "        base = f\"behavior_{tail.replace('.csv','')}\"   # <- agora sempre com prefixo behavior_\n",
    "        by_series[base].append(p)\n",
    "\n",
    "    for base, paths in by_series.items():\n",
    "        parts = []\n",
    "        for p in paths:\n",
    "            try:\n",
    "                df = pd.read_csv(p, low_memory=False)\n",
    "                if \"task\" not in df.columns:\n",
    "                    df.insert(0, \"task\", p.parent.name)\n",
    "                if \"source_file\" not in df.columns:\n",
    "                    src = p.name.split(\"__task-\")[0]\n",
    "                    if not src.endswith(\".nwb\"):\n",
    "                        src += \".nwb\"\n",
    "                    df.insert(0, \"source_file\", src)\n",
    "                parts.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"[{base}] Falha lendo {p.name}: {e}\")\n",
    "\n",
    "        if not parts:\n",
    "            print(f\"[{base}] nenhum arquivo válido para consolidar.\")\n",
    "            continue\n",
    "\n",
    "        final = pd.concat(parts, ignore_index=True)\n",
    "        cols = final.columns.tolist()\n",
    "        for first in [\"source_file\", \"task\"]:\n",
    "            if first in cols:\n",
    "                cols = [first] + [c for c in cols if c != first]\n",
    "        final = final[cols]\n",
    "\n",
    "        out = CONSOLIDATED_DIR / f\"consolidated__{base}.csv\"\n",
    "        final.to_csv(out, index=False)\n",
    "        print(f\"[{base}] SALVO: {out} ({len(final):,} linhas)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
